\documentclass{paper}
\usepackage{amsmath}
\usepackage{amssymb}
\usepackage{hyperref}
\hypersetup{pdftex,colorlinks=true,allcolors=blue}
\DeclareMathOperator*{\argmin}{arg\,min}
\DeclareMathOperator*{\argmax}{arg\,max}

\begin{document}

\title{Derivations for the CSM model}
\maketitle

\section{Definition of the model}

A gestalt, a perceptual object, is characterised by a covariance component for the joint distribution of visual neural activity. 

\begin{eqnarray}
p(v \mid g) = \mathcal{N}(v; 0,C_v) \\
C_v = \sum_{k=1}^K g_k U^T_k U_k \label{eq:cv}
\end{eqnarray}
%
where K is the fixed number of possible gestalts in the visual scene and $g_k$ is the strength of the gestalt number $k$, coming from a $K$-dimensional Gamma prior distribution with shape and scale  parameters $\alpha_g$ and $\theta_g$ controlling the sparsity of the prior.

\begin{equation}
p(g) = \textrm{Gam}(g; \alpha_g,\theta_g)
\end{equation}
%
The global contrast of the image patch is encoded by a scalar variable $z$, also coming from a Gamma prior

\begin{equation}
p(z) = \textrm{Gam}(z; \alpha_z,\theta_z)
\end{equation}

The pixel intensities are generated from the neural activity through a set of linear projective field models, possibly Gabor filters, $A$, scaled by the contrast and adding some independent observational noise.

\begin{equation}
p(x \mid v,z) = \mathcal{N}(x; zAv,\sigma_x I)
\end{equation}

\section{Likelihoods}

\subsection{Likelihood of $g$ and $z$}

by intuition:

\begin{equation} \label{eq:like_gz_int}
p(x \mid z,g) = \mathcal{N}(x;0,\sigma_x I + z^2 A \left( \sum_{k=1}^K g_k U_k^T U_k \right)A^T)
\end{equation}
%
by algebraic derivation (see Seq. \ref{seq:like_equiv}):

\begin{eqnarray}
p(x \mid z,g) = \int_{-\infty}^{\infty} p(x,v \mid z,g)  \mathrm{d}v = \frac{1}{z^{D_v} \sqrt{\det(A^TA)}} \mathcal{N}(\frac{1}{z}A^{+}x;0,\frac{\sigma_x}{z^2} (A^TA)^{-1} + \sum_{k=1}^K g_k U_k^T U_k) \\
f(x,z) \equiv \frac{1}{z}A^{+}x, ~ h(z) \equiv  \frac{1}{z^{D_v} } \\
C(z,g) \equiv \frac{\sigma_x}{z^2} (A^TA)^{-1} + \sum_{k=1}^K g_k U_k^T U_k \\
p(x \mid z,g) = \frac{1}{\sqrt{\det(A^TA)}} h(z) \mathcal{N}(f(x,z);0,C(z,g)) \label{eq:like_gz_alg}
\end{eqnarray}

\subsection{Log-likelihood of the parameters}

All parameters of the model consist of

\begin{equation}
\zeta = \lbrace \sigma_x, A, U_{1..K}, \alpha_g, \theta_g, \alpha_z, \theta_z \rbrace
\end{equation}
%
for $n$ exchangeable observations of $x$, the likelihood looks like this

\begin{equation}
p(X \mid \zeta) = \prod_{n=1}^N p(x_n \mid \zeta) = \prod_{n=1}^N \iint_{-\infty}^{\infty} p(x_n \mid z,g) p(g) p(z) \mathrm{d}g\mathrm{d}z
\end{equation}
%
approximation of the integrals by samples from the priors $p(g)$ and $p(z)$

\begin{equation}
p(X \mid \zeta) \approx \prod_{n=1}^N \frac{1}{L} \sum_{l=1}^{L}p(x_n \mid z^l,g^l)
\end{equation}

using Eq. \ref{eq:like_gz_alg}

\begin{eqnarray}
p(X \mid \zeta) \approx \left( L \sqrt{\det(A^TA)} \right)^{-N} \prod_{n=1}^N \sum_{l=1}^{L} h(z^l) \mathcal{N}(f(x_n,z^l);0,C(z^l,g^l)) \\
\log p(X \mid \zeta) \approx -N (\log L + \frac{1}{2}\log(\det(A^TA))) + \sum_{n=1}^N \log \left[ \sum_{l=1}^{L} h(z^l) \mathcal{N}(f(x_n,z^l);0,C(z^l,g^l)) \right] \\
h^l \equiv h(z^l), ~ f_n^l \equiv f(x_n,z^l), ~ C^l \equiv C(z^l,g^l) \\
\mathcal{L}_n^l \equiv h^l \mathcal{N}(f_n^l;0,C^l), ~ \mathcal{L}_n \equiv  \sum_{l=1}^{L} \mathcal{L}_n^l \\
\log p(X \mid \zeta) \approx -N (\log L + \frac{1}{2}\log(\det(A^TA))) + \sum_{n=1}^N \log \mathcal{L}_n \label{eq:loglike_alg}
\end{eqnarray}
%
An equivalent way to write this based on Eq. \ref{eq:like_gz_int} is the following

\begin{eqnarray}
p(X \mid \zeta) \approx L^{-N} \prod_{n=1}^N \sum_{l=1}^{L} \mathcal{N}(x_n;0,\sigma_x I + z^{l2} A C_v^l A^T) \\
C_x^l \equiv \sigma_x I + z^{l2} A C_v^l A^T \\
\log p(X \mid \zeta) \approx -N \log L + \sum_{n=1}^N \log \left[ \sum_{l=1}^{L} \mathcal{N}(x_n;0,C_x^l) \right] \label{eq:loglike_int}
\end{eqnarray}

\subsubsection{Derivative w.r.t. $U_{1\dots K}$ }

Using Eq. \ref{eq:loglike_alg}

\begin{equation}
\begin{split}
\frac{\partial \log p(X \mid \zeta)}{\partial \left[ U_k \right]_{i,j}} \approx \sum_{n=1}^N \frac{\partial \log \mathcal{L}_n}{\partial \left[ U_k \right]_{i,j}} = \sum_{n=1}^N \frac{1}{\mathcal{L}_n} \frac{\partial \mathcal{L}_n}{\partial \left[ U_k \right]_{i,j}} = \sum_{n=1}^N \frac{1}{\mathcal{L}_n}  \sum_{l=1}^{L} \frac{\partial \mathcal{L}_n^l}{\partial \left[ U_k \right]_{i,j}} = \\
=  \sum_{n=1}^N \frac{1}{\mathcal{L}_n}  \sum_{l=1}^{L} \textrm{Tr} \left[ \frac{\partial \mathcal{L}_n^l}{\partial C^l} \frac{\partial C^l}{\partial \left[ U_k \right]_{i,j}} \right]
\end{split}
\end{equation}
% 
the derivatives in this formula are the following

\begin{eqnarray}
\begin{split}
\frac{\partial \mathcal{L}_n^l}{\partial C^l} = h^l \frac{\partial}{\partial C^l} \mathcal{N}(f_n^l;0,C^l) = h^l \mathcal{N}(f_n^l;0,C^l) \frac{\partial}{\partial C^l} \log \mathcal{N}(f_n^l;0,C^l) = \\
= -\frac{h^l}{2} \mathcal{N}(f_n^l;0,C^l) \left[ (C^l)^{-1} - (C^l)^{-1} f_n^l (f_n^l)^T (C^l)^{-1} \right]
\end{split} \\
\frac{\partial C(z,g)}{\partial \left[ U_k \right]_{i,j}} = g_k \frac{\partial \left( U_k^T U_k \right)}{\partial \left[ U_k \right]_{i,j}} = g_k \left( U_k^T J^{ij} + J^{ji} U_k \right) \equiv g_k \hat U_k^{ij} \label{eq:cv_deriv}
\end{eqnarray}
%
substituting back to the derivative

\begin{equation}
\begin{split}
\frac{\partial \log p(X \mid \zeta)}{\partial \left[ U_k \right]_{i,j}} \approx \\
\approx  -\frac{1}{2} \sum_{n=1}^N \frac{1}{\mathcal{L}_n}  \sum_{l=1}^{L} h^l g^l_k \mathcal{N}(f_n^l;0,C^l)  \textrm{Tr} \left[ \left[ (C^l)^{-1} - (C^l)^{-1} f_n^l (f_n^l)^T (C^l)^{-1} \right] \hat U_k^{ij} \right] = \\
= -\frac{1}{2} \textrm{Tr} \left[ \sum_{n=1}^N \left(\frac{1}{\mathcal{L}_n}  \sum_{l=1}^{L} h^l g^l_k \mathcal{N}(f_n^l;0,C^l)   \left[ (C^l)^{-1} - (C^l)^{-1} f_n^l (f_n^l)^T (C^l)^{-1} \right] \right) \hat U_k^{ij} \right]
\end{split}
\end{equation}
%
The regularities of the $\hat U_k$ matrices allow us to replace the trace with a much more efficient computation:

\begin{eqnarray}
M_k = -\frac{1}{2} \sum_{n=1}^N \left(\frac{1}{\mathcal{L}_n}  \sum_{l=1}^{L} h^l g^l_k \mathcal{N}(f_n^l;0,C^l)   \left[ (C^l)^{-1} - (C^l)^{-1} f_n^l (f_n^l)^T (C^l)^{-1} \right] \right) \\
\frac{\partial \log p(X \mid \zeta)}{\partial \left[ U_k \right]_{i,j}} \approx \textrm{Tr} \left[M_k \hat U_k^{ij} \right] = \sum_{a=1}^{Dv} \left[ M_k \right]_{j,a} \left[ U_k \right]_{i,a} + \left[ M_k \right]_{a,j} \left[ U_k \right]_{i,a}
\end{eqnarray}
%
also, by substituting back Eq. \ref{eq:cv_deriv} we get

\begin{equation}
\textrm{Tr} \left[M_k \hat U_k^{ij} \right] = \textrm{Tr} \left[M_k U_k^T J^{ij} \right] + \mathrm{Tr} \left[ M_k J^{ji} U_k \right]
\end{equation}
%
that is by equivalences of single-entry matrices and noting that $M_k$ are symmetric

\begin{equation}
\textrm{Tr} \left[M_k \hat U_k^{ij} \right] = \left[U_k M_k^T \right]_{i,j} + \left[ U_k M_k \right]_{i,j} = 2 \left[U_k M_k \right]_{i,j}
\end{equation}
%
yielding

\begin{equation}
\frac{\partial \log p(X \mid \zeta)}{\partial U_k} \approx - U_k \sum_{n=1}^N \left(\frac{1}{\mathcal{L}_n}  \sum_{l=1}^{L} h^l g^l_k \mathcal{N}(f_n^l;0,C^l)   \left[ (C^l)^{-1} - (C^l)^{-1} f_n^l (f_n^l)^T (C^l)^{-1} \right] \right)
\end{equation}

\subsubsection{Derivative w.r.t. $\sigma_x$}

Using Eq. \ref{eq:loglike_int}

\begin{equation}
\begin{split}
\frac{\partial \log p(X \mid \zeta)}{\partial \sigma_x} \approx \sum_{n=1}^N \frac{\partial}{\partial \sigma_x} \log \left[ \sum_{l=1}^{L} \mathcal{N}(x_n;0,C_x^l) \right] = \\
= \sum_{n=1}^N \frac{1}{\sum_{l=1}^{L} \mathcal{N}(x_n;0,C_x^l)} \sum_{l=1}^{L} \frac{\partial}{\partial \sigma_x} \mathcal{N}(x_n;0,C_x^l) = \\
= \sum_{n=1}^N \frac{1}{\sum_{l=1}^{L} \mathcal{N}(x_n;0,C_x^l)} \sum_{l=1}^{L} \textrm{Tr} \left[ \frac{\partial}{\partial C_x^l} \mathcal{N}(x_n;0,C_x^l) \frac{\partial C_x^l}{\partial \sigma_x} \right]
\end{split}
\end{equation}

the derivatives in this formula are the following

\begin{eqnarray}
\begin{split}
\frac{\partial}{\partial C_x^l} \mathcal{N}(x_n;0,C_x^l) = \mathcal{N}(x_n;0,C_x^l) \frac{\partial}{\partial C^l} \log \mathcal{N}(x_n;0,C_x^l) = \\
= -\frac{1}{2} \mathcal{N}(x_n;0,C_x^l) \left[ (C_x^l)^{-1} - (C_x^l)^{-1} x_n x_n^T (C_x^l)^{-1} \right]
\end{split} \\
\frac{\partial C_x^l}{\partial \sigma_x} = I
\end{eqnarray}
%
substituting back to the derivative

\begin{equation}
\begin{split}
\frac{\partial \log p(X \mid \zeta)}{\partial \sigma_x} \approx \\
\approx  -\frac{1}{2} \sum_{n=1}^N \frac{1}{\sum_{l=1}^{L} \mathcal{N}(x_n;0,C_x^l)}  \sum_{l=1}^{L} \mathcal{N}(x_n;0,C_x^l)  \textrm{Tr} \left[ (C_x^l)^{-1} - (C_x^l)^{-1} x_n x_n^T (C_x^l)^{-1} \right]
\end{split}
\end{equation}


\subsection{Complete-data log-likelihood}

\begin{equation}
p(V,G,Z,X \mid \zeta) = \prod_{n=1}^N p(x_n \mid v_n,z_n) p(v_n \mid g_n) p(g_n)  p(z_n)
\end{equation}
%
the logarithm of this will be

\begin{equation}
\begin{split}
& \log p(V,G,Z,X \mid \zeta )= \\ 
& = \sum_{n=1}^N  \left[ \log p(g_n) + \log p(z_n) +  \log p(x_n \mid v_n) + \log p(v_n \mid g_n) \right] = \\
& = N  (\log p(g) + \log p(z)) + \sum_{n=1}^N \log p(x_n \mid v_n) + \log p(v_n \mid g_n) = \\
& = c + \sum_{n=1}^N \log p(v_n\mid g_n)
\end{split}
\end{equation}
%
where $c$ is constant with respect to the parameters $U_{1 \dots K}$.


\subsubsection{Expectation w.r.t. the posterior}

\begin{equation}
\mathcal{L}=\iiint_{-\infty}^{\infty} p(V,G,Z \mid X)\log p(V,G,Z,X \mid \zeta) \mathrm{d}V\mathrm{d}G\mathrm{d}Z. 
\end{equation}

We can approximate this integral by averaging over $L$ samples from the full posterior, separately for each observation $x_n$, as defined in Seq. \ref{seq:fullpost}. As we will seek the values of the Cholesky components $U_{1 \dots K}$ that maximise this integral, we can discard each term not depending on these parameters, only leaving the term of the form $p(v \mid g)$. This way we arrive to the following expression 

\begin{equation}
\begin{split}
\mathcal{L} \sim \sum_{n=1}^N \frac{1}{L} \sum_{l=1}^L -\frac{1}{2} \left[\log \left( \det \left( C_v^{(l,n)} \right) \right) + v^{(l,n)T}  \left( C_v^{l,n} \right)^{-1} v^{l,n}\right]& = \\
= -\frac{1}{2L} \sum_{m=1}^{NL} \left[ \log \left( \det \left( C_v^{m} \right) \right) + v^{mT}  \left( C_v^{m} \right)^{-1} v^m \right]&
\end{split}
\end{equation}
%
noting that the double summation over $L$ samples over all $N$ observations always happens on the same terms, so we can substitute it with a single sum that iterates over the full sample set.


\subsubsection{Derivative w.r.t. $U_{1\dots K}$}

Using the derivative of $C_v$ with respect to $U_k^{ij}$ as defined in Eq. \ref{eq:cv_deriv}, by the chain rule, the derivative of $\mathcal{L}$ according to an element of $U_k$ looks like this

\begin{equation}
\begin{split}
&\frac{\partial \mathcal{L}}{\partial \left[ U_k \right]_{i,j}} = -\frac{1}{2L} \sum_{m=1}^{NL} \textrm{Tr} \left[ \frac{\partial \mathcal{L}^m}{\partial C_v^m} \frac{\partial C_v^m}{\partial \left[ U_k \right]_{i,j}} \right] = \\
& -\frac{1}{2L} \sum_{m=1}^{LN} \textrm{Tr} \left[  \left[ \left( C_v^m \right)^{-1} - \left( C_v^m \right)^{-1} v^m v^{mT} \left( C_v^m \right)^{-1} \right] g_k^{m} \hat U_k^{ij} \right] = \\
& -\frac{1}{2L} \textrm{Tr} \left[ \sum_{m=1}^{LN} g_k^{m} \left[ \left( C_v^m \right)^{-1} -  \left( C_v^m \right)^{-1} \left( v^m v^{mT} \right) \left( C_v^m \right)^{-1} \right] \hat U_k^{ij} \right]
\end{split}
\end{equation}
%
The regularities of the $\hat U_k$ matrices allow us to replace the trace with a much more efficient computation:

\begin{eqnarray}
M_k = -\frac{1}{2L} \left[ \sum_{m=1}^{LN} g_k^{m} \left[ \left( C_v^m \right)^{-1} -  \left( C_v^m \right)^{-1} \left( v^m v^{mT} \right) \left( C_v^m \right)^{-1} \right]\right] \label{eq:cdll_deriv_mat} \\
\frac{\partial \mathcal{L}}{\partial \left[ U_k \right]_{i,j}} = \sum_{a=1}^{Dv} \left[ M_K \right]_{j,a} \left[ U_k \right]_{i,a} + \left[ M_k \right]_{a,j} \left[ U_k \right]_{i,a}
\end{eqnarray}

\section{Posteriors}

\subsection{Full posterior} \label{seq:fullpost}

\begin{equation}
p(v,g,z \mid x) = p(x \mid v,g,z) p(v \mid g) p(g) p(z) \frac{1}{p(x)} \sim  p(x \mid v,z) p(v \mid g) p(g) p(z)
\end{equation}
%
so the log-posterior will be the following, up to an additive constant, using Gamma priors over $g$ and $z$ defined by shape and scale parameters:

\begin{equation}
\begin{split}
\log p(v,g,z \mid x) \sim \log p(x \mid v,z) + \log p(v \mid g) + \log p(g) + \log p(z) = \\
= \log \mathcal{N}(x;zAv,\sigma_x I) + \log \mathcal{N}(v;0,C_v) + \log \textrm{Gam}(g;\alpha_g,\theta_g) + \log \textrm{Gam}(z;\alpha_z,\theta_z)
\end{split}
\end{equation}
%
using the logarithms of the used pdfs from Sec. \ref{sec:logpdf} and discarding all terms not dependent on any of the three variables we get

\begin{equation}
\begin{split}
\log p(v,g,z \mid x) \sim -\frac{1}{2 \sigma_x}(x - zAv)^T (x - zAv) - \\
- \frac{1}{2} \left[ \log \left( \det \left( C_v \right) \right) + v^T C_v^{-1} v \right] + \\
+ \sum_{j=1}^K \left[ (\alpha_g - 1) \log(g_j) - \frac{g_j}{\theta_g} \right] + (\alpha_z - 1) \log(z) - \frac{z}{\theta_z}
\end{split}
\end{equation}
%
rearranging the first quadratic term according to Eq. \ref{eq:quad_rearrange} and discarding the term not dependent on $v$ yields

\begin{equation}
\begin{split}
\log p(v,g,z \mid x) \sim - \frac{z}{2 \sigma_x} \left( zv^TA^TAv - 2x^TAv \right)- \\
- \frac{1}{2} \left[ \log \left( \det \left( C_v \right) \right) + v^T C_v^{-1} v \right] + \\
+ \sum_{j=1}^K \left[ (\alpha_g - 1) \log(g_j) - \frac{g_j}{\theta_g} \right] + \\
+ (\alpha_z - 1) \log(z) - \frac{z}{\theta_z}
\end{split}
\end{equation}


\subsubsection{Derivative w.r.t. $v$}

\begin{equation}
\log p(v,g,z \mid x) \sim - \frac{1}{2} \left[ \frac{z^2}{ \sigma_x} v^TA^TAv - \frac{2z}{ \sigma_x}  x^TAv +  v^T C_v^{-1} v \right]  + f_1(g,z)
\end{equation}
%
lumping the two quadratic forms together

\begin{equation}
\log p(v,g,z \mid x) \sim \frac{z}{ \sigma_x}  x^TAv -  \frac{1}{2} v^T \left[ \frac{z^2}{\sigma_x}A^TA + C_v^{-1} \right] v + f_1(g,z)
\end{equation}
%
Taking the derivative using Eq. \ref{eq:deriv_quadratic} and \ref{eq:deriv_scalarprod} we get

\begin{equation}
\frac{\partial}{\partial v} \log p(v,g,z \mid x) =  \frac{z}{ \sigma_x}  A^Tx - \left[ \frac{z^2}{\sigma_x}A^TA + C_v^{-1} \right] v 
\end{equation}


\subsubsection{Derivative w.r.t. $g$}

\begin{equation}
\begin{split}
\log p(v,g,z \mid x) \sim - \frac{1}{2} \left[ \log \det \left( C_v \right) + v^T C_v^{-1} v \right] + \\
+ \sum_{j=1}^K \left[ (\alpha_g - 1) \log(g_j) - \frac{g_j}{\theta_g} \right]   + f_2(v,z)
\end{split}
\end{equation}
%
Taking the derivative w.r.t. a single $g_i$ using Eq. \ref{eq:deriv_chain} we get

\begin{equation}
\begin{split}
\frac{\partial}{\partial g_i} \log p(v,g,z \mid x) =  - \frac{1}{2} \textrm{Tr} \left[ \frac{\partial}{\partial C_v} \left[ \log \det \left( C_v \right) + v^T C_v^{-1} v \right] \frac{\partial C_v}{\partial g_i}\right]  + \\
+ \frac{\partial}{\partial g_i} \left[ (\alpha_g - 1) \log(g_i) - \frac{g_i}{\theta_g} \right]
\end{split}
\end{equation}
%
using Eq. \ref{eq:deriv_logdet}, \ref{eq:deriv_quad_mat} and \ref{eq:deriv_scalar} we arrive to

\begin{equation}
\frac{\partial}{\partial g_i} \log p(v,g,z \mid x) =  - \frac{1}{2} \textrm{Tr} \left[ \left[ C_v^{-1} - C_v^{-1} v v^T C_v^{-1} \right] C_i \right]  + \frac{\alpha_g - 1}{g_i} - \frac{1}{\theta_g}
\end{equation}


\subsubsection{Derivative w.r.t. $z$}

\begin{equation}
\log p(v,g,z \mid x) \sim - \frac{z}{2 \sigma_x} \left( zv^TA^TAv - 2x^TAv \right) + (\alpha_z - 1) \log(z) - \frac{z}{\theta_z} + f_3(g,v)
\end{equation}

\begin{equation}
\frac{\partial}{\partial z} \log p(v,g,z \mid x) =  \frac{1}{\sigma_x} \left[ x^TAv - z v^T A^TA v \right] + \frac{\alpha_z - 1}{z} - \frac{1}{\theta_z}
\end{equation}

\subsection{Conditional posteriors}

\subsubsection{Conditional posterior of $v$}

\begin{equation} \label{eq:condpost}
p(v \mid x,g,z) = \frac{p(x \mid v,z,g) p(v \mid z,g)}{p(x \mid z,g)} = \frac{\mathcal{N}(x;zAv,\sigma_x I) \mathcal{N}(v;0,C_v)}{\int_{-\infty}^{\infty} \mathcal{N}(x;zAv,\sigma_x I) \mathcal{N}(v;0,C_v) \mathrm{d}v}
\end{equation}
%
the product of two Gaussians in the numerator of Eq. \ref{eq:condpost} can also be written as a Gaussian over $v$ as in Seq. \ref{sec:mergegauss}:

\begin{equation} \label{eq:gauss_rewrite2}
\mathcal{N}(x;zAv,\sigma_x I) \mathcal{N}(v;0,C_v) = c \mathcal{N}(v; \mu_{post},C_{post})
\end{equation}
%
The denominator of Eq. \ref{eq:condpost} is the integral of this formula, which evaluates to $c$, as the Gaussian integrates to one. This cancels the constant in the numerator, making the conditional posterior equal to the combined Gaussian over $v$, which, after expanding $\mu_{post}$ and $C_{post}$, is

\begin{equation} \label{eq:condpost_v}
p(v \mid x,g,z) = \mathcal{N}\left(v; \frac{z}{ \sigma_x} \left( \frac{z^2}{ \sigma_x} A^T A + C_v^{-1}\right)^{-1} A^T x, \left(\frac{z^2}{\sigma_x} A^T A + C_v^{-1}\right)^{-1}\right)
\end{equation}


\subsubsection{Conditional posterior of $g$}

\begin{equation} 
p(g \mid X,V,z) = \frac{p(X \mid g,V,z) p(g \mid V,z)}{p(X \mid V,z)} = \frac{p(V \mid g) p(g)}{p(V)}
\end{equation}
%
taking the logarithm and discarding constant terms

\begin{equation} \label{eq:g_cond_logpost}
\log p(g \mid X,V) \sim -\frac{1}{2} \left[ \log(\det(C_v)) + v^T C_v^{-1} v\right] + \log p(g)
\end{equation}
%
The unnormalised conditionals of single elements of $g$, assuming an independent prior look as follows

\begin{equation} 
\begin{split}
\log p(g_j \mid g_{\neg j},X,V) = \frac{p(V \mid g_j,g_{\neg j},X) p(g_j \mid g_{\neg j},X)}{p(V \mid g_{\neg_j},X)} = \\
= \frac{ p(V \mid g) p(g_j) }{ p(V \mid g_{\neg_j}) } \sim p(V \mid g) p(g_j)
\end{split}
\end{equation}


\subsubsection{Conditional posterior of $z$}

\begin{equation} 
p(z \mid X,V,g) = \frac{p(X \mid g,z,V) p(z \mid V,g)}{p(X \mid V,g)} \sim p(X \mid z,V) p(z)
\end{equation}
% 
the log-posterior being

\begin{equation} \label{eq:z_cond_logpost}
\log p(z \mid X,V) \sim -\frac{1}{2} \left[ D_x\log(\sigma_x) + \frac{1}{\sigma_x} (x - zAv)^T (x - zAv)\right] + \log p(z)
\end{equation}


\subsection{Marginal posteriors}

\subsubsection{Marginal posterior of $g$ and $z$}

\begin{equation}
p(g,z \mid x) \sim p(x \mid g,z)p(g)p(z)
\end{equation}
%
from Eq. \ref{eq:like_gz_int}

\begin{eqnarray}
\begin{split}
\log p(g,z \mid x) \sim -\frac{1}{2} \left[ \log \det (C_x) + x^T C_x^{-1} x \right] + (\alpha_z-1)\log(z) - \frac{z}{\theta_z} + \\
+ (\alpha_g-1)\sum_{k=1}^K \log(g_k) - \frac{1}{\theta_g} \sum_{k=1}^K g_k
\end{split} \\
C_x = \sigma_x I + z^2 A \left( \sum_{k=1}^K g_k U_k^T U_k \right)A^T
\end{eqnarray}
%
the derivative w.r.t $g_k$

\begin{equation}
\begin{split}
\frac{\partial}{\partial g_k} \log p(g,z \mid x) = \mathrm{Tr} \left[ \frac{\partial}{\partial C_x} \log p(g,z \mid x) \frac{\partial C_x}{\partial g_k} \right] + \frac{\alpha_g-1}{g_k} - \frac{1}{\theta_g} = \\
= -\frac{z^2}{2} \mathrm{Tr} \left[ \left[ C_x^{-1} - C_x^{-1} xx^T C_x^{-1} \right] AU_k^TU_kA^T \right] + \frac{\alpha_g-1}{g_k} - \frac{1}{\theta_g}
\end{split}
\end{equation}
%
the derivative w.r.t $z$

\begin{equation}
\begin{split}
\frac{\partial}{\partial z} \log p(g,z \mid x) = \mathrm{Tr} \left[ \frac{\partial}{\partial C_x} \log p(g,z \mid x) \frac{\partial C_x}{\partial z} \right] + \frac{\alpha_z-1}{z} - \frac{1}{\theta_z} = \\
= -z \mathrm{Tr} \left[ \left[ C_x^{-1} - C_x^{-1} xx^T C_x^{-1} \right] A C_v A^T \right] + \frac{\alpha_z-1}{z} - \frac{1}{\theta_z}
\end{split}
\end{equation}


\subsubsection{Marginal posterior of $g$}

A maximum a posterior estimate of $g$ can be given as follows

\begin{eqnarray}
g_{MAP} = \argmax_g p(g \mid x) = \argmax_g \frac{p(x \mid g)p(g)}{p(x)} = \argmax_g p(x \mid g)p(g)\\
p(x \mid g) = \int_{-\infty}^{\infty} p(x \mid z,g)p(z)\mathrm{d}z \approx \frac{1}{L} \sum_{l=1}^{L} p(x \mid g,z^l)
\end{eqnarray}


\subsubsection{Marginal posterior of $v$}

\begin{eqnarray}
p(v \mid x) = \iint_{-\infty}^\infty p(v \mid x,g,z) p(g,z \mid x) \mathrm{d}g \mathrm{d}z \\
p(v \mid x) \approx \frac{1}{L} \sum_{l=1}^L p(v \mid x,g^l,z^l), ~~ g^l, z^l \sim p(g,z \mid x)
\end{eqnarray}
%
where $p(v \mid x,g,z)$ is given by Eq. \ref{eq:condpost_v}, so we approximate the marginal posterior with a finite mixture of Gaussians, for wich the covariance is given in the following form

\begin{eqnarray}
C_{v \mid x} \approx \frac{1}{L} \sum_{l=1}^L C_{v \mid xgz}^l + ( \mu_{v \mid xgz}^l - \frac{1}{L} \sum_{m=1}^L \mu_{v \mid xgz}^m) ( \mu_{v \mid xgz}^l - \frac{1}{L} \sum_{m=1}^L \mu_{v \mid xgz}^m)^T \\
C_{v \mid x} \approx \mathrm{E} \left[ C_{v \mid xgz}^l \right]_l + \mathrm{Cov} \left[\mu_{v \mid xgz}^l \right]_l \\
C_{v \mid xgz} =  \left(\frac{z^2}{\sigma_x} A^T A + \left[\sum_{k=1}^K g_k C_k \right]^{-1}\right)^{-1} \\
\mu_{v \mid xgz} = \frac{z}{\sigma_x} C_{v \mid xgz} A^T x
\end{eqnarray}
%
The mean of the finite mixture is given by

\begin{equation}
\mu_{v \mid x} \approx \frac{1}{L} \sum_{l=1}^L \mu_{v \mid xgz}^l
\end{equation}

\section{Appendix}

\subsection{Rules of differentiation}

Assuming that $y$ and $a$ are vectors, $S$ is a symmetric matrix of appropriate dimension, $M$ is any matrix, $f$ is a scalar function, and $\gamma$ is a scalar variable.

\begin{eqnarray}
\frac{\partial}{\partial y} y^T S y = 2 S y \label{eq:deriv_quadratic} \\
\frac{\partial}{\partial y} a^T y = a \label{eq:deriv_scalarprod} \\
\frac{\partial}{\partial S} y^T S^{-1} y = - S^{-1} yy^T S^{-1} \label{eq:deriv_quad_mat} \\
\frac{\partial}{\partial S} \log \det S = S^{-1} \label{eq:deriv_logdet} \\
\frac{\partial}{\partial \gamma} f(S(\gamma)) = \textrm{Tr} \left[ \frac{\partial f}{\partial S} \frac{\partial S}{\partial \gamma} \right] \label{eq:deriv_chain} \\
\frac{\partial}{\partial \gamma} \gamma S = S \label{eq:deriv_scalar} \\
\frac{\partial}{\partial \gamma} f(\gamma) = f(\gamma) \frac{\partial}{\partial \gamma} \log f(\gamma) \label{eq:deriv_function} \\
\frac{\partial}{\partial S} \log \mathcal{N}(y;a,S) = -\frac{1}{2} \left[ S^{-1} - S^{-1}(y-a)(y-a)^TS^{-1} \right]\label{eq:deriv_gausscov} \\
\frac{\partial}{\partial a} (y-Ma)^T S (y-Ma) = -2 M^T S (y-Ma) \label{eq:quadderiv}
\end{eqnarray}


\subsection{Logarithms of used PDFs} \label{sec:logpdf}

\begin{eqnarray}
\log  \mathcal{N}(y;\mu,C) = -\frac{1}{2} \left[D \log (2\pi) + \log \det (C) + (y - \mu)^T C^{-1} (y-\mu) \right] \\
\log  \textrm{Gam}(y;\alpha,\zeta) = \log(1) - \log(\Gamma(\alpha)) - \alpha \log(\zeta) + (\alpha-1) \log(y) - \frac{y}{\zeta}
\end{eqnarray}

\subsection{Switching and transforming variables of a Gaussian} \label{sec:varswitch}

We want to merge two Gaussians over $x$ and $v$ into one over $v$

\begin{equation}
p(x \mid v,z) p(v \mid g) = \mathcal{N}(x;zAv,\sigma_x I) \mathcal{N}(v;0,C_v)
\end{equation}
%
The Gaussian over $x$ spelled out is

\begin{equation} 
\mathcal{N}(x;zAv,\sigma_x I) = \sqrt{\frac{1}{(2\pi)^{Dx} \sigma_x^{Dx}}}e^{-\frac{1}{2 \sigma_x} (x-zAv)^T(x-zAv)}
\end{equation}
%
rearranging the quadratic term:

\begin{equation} \label{eq:quad_rearrange}
\begin{split}
-\frac{1}{2} (x-zAv)^T(x-zAv) = -\frac{1}{2} (x^Tx - zv^TA^Tx - zx^TAv + z^2 v^TA^TAv) = \\
= -\frac{1}{2} (x-zAv)^T(x-zAv) = -\frac{1}{2} (x^Tx - 2zx^TAv + z^2 v^TA^TAv) = \\
= -\frac{x^Tx}{2} + zx^TAv -\frac{z^2}{2} v^TA^TAv
\end{split}
\end{equation}
%
as $v^TA^Tx = (x^TAv)^T$, and both are scalars, thus equal to their transposes, it's also true that $v^TA^Tx = x^TAv$. We have the identity for any symmetric matrix $M$ and vector $b$ that

\begin{equation} 
-\frac{1}{2} v^T M v + b^Tv = -\frac{1}{2} (v - M^{-1}b)^T M (v - M^{-1}b) + \frac{1}{2}b^T M^{-1} b
\end{equation}
%
making the substitution $M = z^2A^TA$ and $b = (zx^TA)^T=zA^Tx$, yielding $M^{-1} = \frac{1}{z^2}(A^TA)^{-1}$ and $M^{-1} b = \frac{1}{z}(A^TA)^{-1}A^Tx = \frac{1}{z}A^{+}x$, where $A^{+}$ is the Moore-Penrose pseudoinverse of $A$. Thus we get

\begin{equation}
\begin{split}
-\frac{1}{2} (x-zAv)^T(x-zAv) = \\
= -\frac{x^Tx}{2} -\frac{1}{2} (v - \frac{1}{z}A^{+}x)^T z^2A^TA (v - \frac{1}{z}A^{+}x)  + \frac{1}{2} (A^Tx)^T (A^TA)^{-1}A^Tx = \\
=-\frac{x^Tx}{2} -\frac{1}{2} (v - \frac{1}{z}A^{+}x)^T z^2A^TA (v - \frac{1}{z}A^{+}x)  + \frac{1}{2} x^T A A^{-1} A^{-T} A^T x = \\
= -\frac{x^Tx}{2} -\frac{1}{2} (v - \frac{1}{z}A^{+}x)^T z^2A^TA (v - \frac{1}{z}A^{+}x)  + \frac{x^Tx}{2} = \\
= -\frac{1}{2} (v - \frac{1}{z}A^{+}x)^T z^2A^TA (v - \frac{1}{z}A^{+}x)
\end{split}
\end{equation}
%
which implies

\begin{equation}
e^{-\frac{1}{2 \sigma_x} (x-zAv)^T(x-zAv)} = e^{-\frac{1}{2} (v - \frac{1}{z}A^{+}x^T)^T \frac{z^2}{\sigma_x} A^TA(v - \frac{1}{z}A^{+}x)}
\end{equation}
% 
meaning that

\begin{equation} 
\mathcal{N}(x;zAv,\sigma_x I) = \alpha \mathcal{N}(v;\frac{1}{z}A^{+}x,\frac{\sigma_x}{z^2} (A^TA)^{-1})
\end{equation}
%
and as the formulas in the exponents are equal, the constant  $\alpha$ is given by the ratio of the normalisation terms

\begin{eqnarray}
\sqrt{\frac{1}{(2\pi)^{Dx} \sigma_x^{Dx}}} = \alpha \sqrt{\frac{1}{(2\pi)^{Dv} \det(\frac{\sigma_x}{z^2} (A^TA)^{-1})}} \\
\alpha = \sqrt{\frac{ (2\pi)^{Dv} \frac{\sigma_x^{D_v}}{z^{2D_v}} \det( (A^TA)^{-1}) }{ (2\pi)^{Dx} \sigma_x^{Dx} }} \\
\alpha = \sqrt{\frac{ (2\pi)^{Dv} \sigma_x^{D_v} }{ (2\pi)^{Dx} \sigma_x^{Dx} z^{2D_v} \det(A^TA)}}
\end{eqnarray}
%
making the simplifying assumption $D_x = D_v$ we arrive to

\begin{equation} 
\mathcal{N}(x;zAv,\sigma_x I) = \frac{1}{\sqrt{\det(A^TA)}} \frac{1}{z^{D_v}} \mathcal{N}(v;\frac{1}{z}A^{+}x,\frac{\sigma_x}{z^2} (A^TA)^{-1})
\end{equation}

\subsection{Merging two Gaussian distributions} \label{sec:mergegauss}

\begin{equation} 
\mathcal{N}(v;\mu_1,C_1) \mathcal{N}(v;\mu_2,C_2) = \mathcal{N}(\mu_1;\mu_2,C_1 + C_2) \mathcal{N}(v; \mu_c,C_c)
\end{equation}
%
where $C_c = (C_1^{-1} + C_2^{-1})^{-1}$ and $\mu_c = C_c (C_1^{-1}\mu_1 + C_2^{-1}\mu_2)$. Substitution to these formulas yields

\begin{eqnarray}
\begin{split}
 \frac{1}{\sqrt{\det(A^TA)}} \frac{1}{z^{D_v}} \mathcal{N}(v;\frac{1}{z}A^{+}x,\frac{\sigma_x}{z^2} (A^TA)^{-1})\mathcal{N}(v;0,C_v) = \\
\frac{1}{\sqrt{\det(A^TA)}} \frac{1}{z^{D_v}} \mathcal{N}(\frac{1}{z}A^{+}x;0,\frac{\sigma_x}{z^2} (A^TA)^{-1} + C_v) \mathcal{N}(v; \mu_c,C_c)
 \end{split} \\
 C_c = (\frac{z^2}{\sigma_x} (A^TA) + C_v^{-1})^{-1} \\
 \mu_c = C_c \frac{z}{\sigma_x} (A^TA) A^{+}x = \frac{z}{\sigma_x} C_c A^{T}x
\end{eqnarray}


\subsection{Equivalence of the two likelihood formulas of CSM} \label{seq:like_equiv}

Expanding \ref{eq:like_gz_alg} yields

\begin{eqnarray}
\begin{split}
p(x \mid z,g) = \frac{1}{\sqrt{\det(A^TA)}} \frac{1}{z^{D_v}} \frac{1}{\sqrt{ (2\pi)^{D_v} \det C(z,g) }} e^{-\frac{1}{2} \left( \frac{1}{z}A^{+}x \right)^T C^{-1}(z,g) \frac{1}{z}A^{+}x}  = \\
= \frac{1}{\sqrt{ (2\pi z^2)^{D_v} \det(A^TA C(z,g))}} e^{-\frac{1}{2z^2} x^TA^{+T} C^{-1}(z,g) A^{+}x}
\end{split} \\
C(z,g) \equiv \frac{\sigma_x}{z^2} (A^TA)^{-1} + C_v
\end{eqnarray}
%
so in the exponent, in the place of the covariance matrix, we have

\begin{equation}
\begin{split}
\frac{1}{z^2} \left( (A^TA)^{-1}A^T \right)^T \left[  \frac{\sigma_x}{z^2} (A^TA)^{-1} + C_v  \right]^{-1} (A^TA)^{-1}A^T = \\
= \frac{1}{z^2} A (A^TA)^{-1} \left[  \frac{\sigma_x}{z^2} (A^TA)^{-1} + C_v  \right]^{-1} (A^TA)^{-1}A^T = \\
= \frac{1}{z^2} A \left[ \left[ \frac{\sigma_x}{z^2} (A^TA)^{-1} + C_v \right] (A^TA) \right]^{-1} (A^TA)^{-1}A^T = \\
= \frac{1}{z^2} A \left[ \frac{\sigma_x}{z^2} I + C_v A^TA \right]^{-1} (A^TA)^{-1}A^T = \\
= \frac{1}{z^2} A \left[ (A^TA) \left[ \frac{\sigma_x}{z^2} I + C_v A^TA \right] \right]^{-1} A^T = \\
= \frac{1}{z^2} A  \left[ \frac{\sigma_x}{z^2} A^TA + A^TA C_v A^TA \right]^{-1} A^T
\end{split}
\end{equation}
%
assuming that $A$ is invertible this is equal to

\begin{equation}
\begin{split}
\frac{1}{z^2} \left[ A^{-1}\right]^{-1}  \left[ \frac{\sigma_x}{z^2} A^TA + A^TA C_v A^TA \right]^{-1} \left[ A^{-T}\right]^{-1} = \\
\frac{1}{z^2} \left[ A^{-1}\right]^{-1}  \left[ A^{-T} \left[ \frac{\sigma_x}{z^2} A^TA + A^TA C_v A^TA \right] \right]^{-1} = \\
\frac{1}{z^2} \left[ A^{-1}\right]^{-1}  \left[ \frac{\sigma_x}{z^2} A + A C_v A^TA \right]^{-1} = \\
\frac{1}{z^2} \left[  \left[ \frac{\sigma_x}{z^2} A + A C_v A^TA\right] A^{-1} \right]^{-1} = \\
\frac{1}{z^2} \left[ \frac{\sigma_x}{z^2} I + A C_v A^T\right]^{-1} = \\
= \left[ \sigma_x I + z^2 A C_v A^T \right]^{-1}
\end{split}
\end{equation}
%
under the square root we have

\begin{equation}
\begin{split}
z^{2 D_v} \det(A^TA C(z,g)) = z^{2 D_v} \det(A^T) \det(A) \det(C(z,g)) = z^{2 D_v} \det(A) \det(C(z,g)) \det(A^T)\\
= z^{2 D_v} \det(A) \det(\frac{\sigma_x}{z^2} (A^TA)^{-1} + C_v) \det(A^T) = 
z^{2 D_v} \det(A) \det(\frac{\sigma_x}{z^2} (A^TA)^{-1} + C_v) \det(A^T) = \\
= z^{2 D_v} \det(A) \det(\frac{\sigma_x}{z^2} A^{-1}A^{-T} + C_v) \det(A^T) = 
z^{2 D_v} \det \left[ A \left[ \frac{\sigma_x}{z^2} A^{-1}A^{-T} + C_v \right] A^T \right] = \\
= z^{2 D_v} \det \left[  \frac{\sigma_x}{z^2} I + A C_v A^T \right] =
\det \left[  \sigma_x I + z^2 A C_v A^T \right]
\end{split}
\end{equation}


\subsection{Precision component formulation of the CSM model}

The model can be equally well parametrised by precision components

\begin{eqnarray}
p(v \mid g) = \mathcal{N}(v; 0,\Lambda_v^{-1}) \\
\Lambda_v = \sum_{k=1}^K g_k \Lambda_k \label{eq:cv}
\end{eqnarray}
%
in this case the conditional posterior over $v$ takes the form

\begin{equation}
p(v \mid x,g) = \mathcal{N}\left(v; \frac{1}{\sigma_x} \left(\frac{1}{\sigma_x} A^T A + \Lambda_v \right)^{-1} A^T x, \left(\frac{1}{\sigma_x} A^T A + \Lambda_v\right)^{-1}\right)
\end{equation}
%
and the conditional posterior of $g$ will look as follows
\begin{equation} 
\log p(g \mid X,V) \sim -\frac{1}{2} \left[ \log(\det(\Lambda_v^{-1})) + v^T \Lambda_v v\right] + \log p(g)
\end{equation}
%
The gradient of the expectation of the complete-data log-likelihod with respect to the joint posterior will look like this

\begin{eqnarray}
\Lambda_k = U_k^T U_k \\
\frac{\partial \mathcal{L}}{\partial \left[ U_k \right]_{i,j}} = \frac{1}{L} \sum_{m=1}^{LN} g_k^{m} \textrm{Tr} \left[ \left[ \left( \Lambda_v^m \right)^{-1} - v^m v^{mT} \right] \hat U_k^{ij} \right]
\end{eqnarray}

\subsection{Batches of observations}

For a single set of component activations $g$ and contrast $z$, we might have a batch of $v$ and $x$ values of size $B$. This modifies expressions as follows.

Conditional posterior of $g$ (Eq. \ref{eq:g_cond_logpost})

\begin{equation} 
\log p(g \mid X,V) \sim -\frac{1}{2} \left[B\log(\det(C_v)) + \sum_{b=1}^B v_b^T C_v^{-1} v_b\right] + \log p(g)
\end{equation}
%
Conditional posterior of $z$ (Eq. \ref{eq:z_cond_logpost})

\begin{equation} 
\log p(z \mid X,V) \sim -\frac{1}{2} \left[B D_x\log(\sigma_x) + \frac{1}{\sigma_x} \sum_{b=1}^B (x_b - zAv_b)^T (x_b - zAv_b)\right] + \log p(z)
\end{equation}
Matrix formula in the derivative of the expectation of the complete-data log-likelihood (Eq. \ref{eq:cdll_deriv_mat})

\begin{equation}
M_k = -\frac{1}{2L} \left[ \sum_{m=1}^{LN} g_k^{m} \left[ B \left( C_v^m \right)^{-1} -  \left( C_v^m \right)^{-1} \left( \sum_{b=1}^B v^{m,b} v^{(m,b)T} \right) \left( C_v^m \right)^{-1} \right]\right]
\end{equation}

\subsection{Latents affecting the mean}

of $v$, instead of the covariance

\begin{eqnarray}
p_m(v \mid g) = \mathcal{N}(v; Bg,\sigma_v I) \\
p_m(v \mid x,g,z) = \mathcal{N}(v;\mu_m,C_m) \\
C_m =  \left(\frac{z^2}{\sigma_x} A^T A + \frac{1}{\sigma_v} I \right)^{-1} \\
\mu_m = C_m \left(\frac{z}{\sigma_x} A^T x +  \frac{1}{\sigma_v} B g \right)
\end{eqnarray}

likelihood of $g$ and $z$, by intuition:

\begin{equation}
p_m(x \mid g,z) = \mathcal{N}(x; zABg, \sigma_x I + z^2 \sigma_v AA^T)
\end{equation}
%
log-posterior of $g$ and $z$

\begin{eqnarray}
\begin{split}
\log p_m(g,z \mid x) \sim -\frac{1}{2} \left[ \log \det (C_{xm}) + (x-zABg)^T C_{xm}^{-1} (x-zABg) \right] + \\
+ (\alpha_z-1)\log(z) - \frac{z}{\theta_z} + (\alpha_g-1)\sum_{k=1}^K \log(g_k) - \frac{1}{\theta_g} \sum_{k=1}^K g_k
\end{split} \\
C_{xm} = \sigma_x I + z^2 \sigma_v AA^T
\end{eqnarray}
%
the derivative w.r.t. $g$ is given by Eq. \ref{eq:quadderiv}

\begin{equation}
\frac{\partial}{\partial g} \log p_m(g,z \mid x) = zB^TA^T C_{xm}^{-1} (x-zABg) + \frac{\partial}{\partial g} \log p(g)
\end{equation}

%
for the sake of the derivative w.r.t. $z$, we need write out the quadratic form:

\begin{equation}
\begin{split}
\mathcal{L}_m \equiv (x-zABg)^T C_{xm}^{-1} (x-zABg) \equiv (x-zy)^T C_{xm}^{-1} (x-zy) \\
= x^T C_{xm}^{-1} x - 2 x^T C_{xm}^{-1} zy + zy^T C_{xm}^{-1} zy = \\
= x^T C_{xm}^{-1} x - 2 x^T \left[ \frac{1}{z} C_{xm} \right]^{-1} y + y^T  \left[ \frac{1}{z^2} C_{xm} \right]^{-1} y
\end{split}
\end{equation}

\begin{equation}
\begin{split}
\frac{\partial  \mathcal{L}_m}{\partial z}  = \mathrm{Tr} \left[  \frac{\partial}{\partial C_{xm}} x^T C_{xm}^{-1} x  \frac{\partial C_{xm}}{\partial z} \right] 
+ \mathrm{Tr} \left[  \frac{\partial}{\partial \frac{1}{z} C_{xm}}  2 x^T \left[ \frac{1}{z} C_{xm} \right]^{-1} y  \frac{\partial \frac{1}{z} C_{xm}}{\partial z} \right] + \\ 
+  \mathrm{Tr} \left[  \frac{\partial}{\partial \frac{1}{z^2} C_{xm}}   y^T  \left[ \frac{1}{z^2} C_{xm} \right]^{-1} y  \frac{\partial \frac{1}{z^2} C_{xm}}{\partial z} \right]
\end{split}
\end{equation}
%
the six derivatives being

\begin{eqnarray}
\frac{\partial}{\partial C_{xm}} x^T C_{xm}^{-1} x = - C_{xm}^{-1} xx^T C_{xm}^{-1} \\
\frac{\partial}{\partial \frac{1}{z} C_{xm}}  2 x^T \left[ \frac{1}{z} C_{xm} \right]^{-1} y = -2z^2 C_{xm}^{-1} x y^T C_{xm}^{-1} \\
\frac{\partial}{\partial \frac{1}{z^2} C_{xm}}   y^T  \left[ \frac{1}{z^2} C_{xm} \right]^{-1} y = - z^4 C_{xm}^{-1} yy^T C_{xm}^{-1} \\
\frac{\partial C_{xm}}{\partial z} = 2z \sigma_v AA^T \\
 \frac{\partial \frac{1}{z} C_{xm}}{\partial z} =  - \frac{\sigma_x}{z^2} I + \sigma_v AA^T \\
 \frac{\partial \frac{1}{z^2} C_{xm}}{\partial z} = -\frac{2 \sigma_x}{z^3} I
\end{eqnarray}
%
substituting back

\begin{equation}
\begin{split}
\frac{\partial  \mathcal{L}_m}{\partial z}  = \mathrm{Tr} \left[   - C_{xm}^{-1} \left[ 2z \sigma_v AA^T xx^T  - 2 \sigma_x x y^T + 2z^2 \sigma_v AA^T x y^T - 2z \sigma_x yy^T \right] C_{xm}^{-1}  \right]  = \\
=  -2 \mathrm{Tr} \left[ C_{xm}^{-1} \left[ z \sigma_v AA^T (xx^T  + z x y^T) - \sigma_x (x y^T  - z yy^T) \right] C_{xm}^{-1}  \right]
\end{split}
\end{equation}


\end{document}
