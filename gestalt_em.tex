\documentclass{paper}
\usepackage{amsmath}
\usepackage{amssymb}
\DeclareMathOperator*{\argmin}{arg\,min}

\begin{document}

\title{EM for the gestalt model}
\maketitle

\section{Generative model}

A gestalt, a perceptual object, is characterised by a covariance component for the joint distribution of visual neural activity. 

\begin{eqnarray}
p(v \mid g) = \mathcal{N}(v; 0,C_v) \\
C_v = \sum_{k=1}^K g_k C_k \label{eq:cv}
\end{eqnarray}
%
where K is the fixed number of possible gestalts in the visual scene and $g_k$ is the strength of the gestalt number $k$, coming from a $K$-dimensional symmetric Dirichlet prior distribution with concentration parameter $\alpha$ controlling the sparsity of the prior.

\begin{equation}
p(g) = \textrm{Dir}(g; \alpha)
\end{equation}
%
The pixel intensities are generated from the neural activity through a set of linear projective field models, possibly Gabor filters, $A$, adding some independent observational noise.

\begin{eqnarray}
p(x \mid v) = \mathcal{N}(x; Av,C_x) \\
C_x = \sigma_x I;
\end{eqnarray}

We might assume that a single composition of gestalts, characterised by the $g$ vector, generates a batch of $B$ independent images, described by cellular activities $V = \lbrace v_1 \dots v_B \rbrace$ and observations $X = \lbrace x_1 \dots x_B \rbrace$. Then the following likelihood distributions hold

\begin{eqnarray}
p(V \mid g) = \prod_{b=1}^B \mathcal{N}(v_b; 0,C_v) \\
p(X \mid V) = \prod_{b=1}^B \mathcal{N}(x_b; Av_b,C_x)
\end{eqnarray}

\section{E-step}

The joint posterior over hidden variables $g$ and $V$ is the following

\begin{equation}
p(V,g \mid X) = p(X \mid V,g) \frac{p(V,g)}{p(x)} = p(X \mid V) p(V \mid g) p(g)\frac{1}{p(X)}
\end{equation}
%
For the purpose of sampling, we can discard the normalisation factor $p(X)$ and the normalisation constants of the Gaussians. The logarithm of the Dirichlet prior over $g$ looks like the following 

\begin{equation}
\log p(g) = \log(\Gamma(\alpha K)) - \log(\Gamma(\alpha)^K) + (\alpha-1) \sum_{k=1}^K \log(g_k)
\end{equation}

%
We can discard the terms not depending on $g$. So taking the logarithm of the unnormalised posterior, the sampling target will look like this

\begin{equation}
\begin{split}
&\log p(V,g \mid X) \sim \\ 
&-\frac{1}{2} \left[\sum_{b=1}^B \left[(x_b-Av_b)^T C_x^{-1} (x_b-Av_b) + v_b^T C_v^{-1} v_b \right] + B \log(\det(C_v)) \right] + \\
&+ (\alpha-1) \sum_{k=1}^K \log(g_k) \label{eq:log_post}
\end{split}
\end{equation}
%
where $g \in \left(0,1 \right]$ and $\sum_{k=1}^{K} g_k = 1$, and $-\infty$ everywhere else. 

\subsection{Hamiltonian Monte Carlo Sampling}

The negative log-posterior can be regarded as an energy function for a dynamical system updating invariantly to the posterior distribution. To use Hamiltonian MC sampling, the gradient of this energy has to be constructed as follows (for $B=1$)

\begin{eqnarray}
\begin{split}
&E(g,v) = \\
&\frac{1}{2} \left[(x-Av)^T C_x^{-1} (x-Av) + \log(\det(C_v)) + v^T C_v^{-1} v\right] + (\alpha-1) \sum_{k=1}^K \log(g_k)
\end{split}\\
\begin{split}
&\frac{\partial E(g,v)}{\partial g_j} = \\
&\frac{1}{2} \sum_{a=1}^{D_v} \sum_{b=1}^{D_v} \left[ \left( \sum_{k=1}^K g_k C_k \right)^{-1} \left[ I - vv^T \left( \sum_{k=1}^K g_k C_k \right)^{-1} \right] \right]_{a,b} \left[ C_j \right]_{a,b} + \frac{(\alpha-1)}{g_j}
\end{split}\\
\frac{\partial E(g,v)}{\partial v} = A^T C_x^{-1}(Av-x) + \left( \sum_{k=1}^K g_k C_k \right)^{-1} v
\end{eqnarray}
%
denoting the dimension of $v$ as $D_v$ and using differentiating rules \ref{eq:diffrule1}, \ref{eq:diffrule2} and \ref{eq:diffrule3}

\begin{eqnarray}
\frac{\partial}{\partial M} \log(\det(M))= M^{-T} \label{eq:diffrule1}\\
\frac{\partial}{\partial M} a^T M^{-1} a = - M^{-T} aa^T M^{-T} \label{eq:diffrule2}\\
f: \mathbb{R}^{D \times D} \rightarrow \mathbb{R} \;\;\; h: \mathbb{R} \rightarrow \mathbb{R}^{D \times D} \nonumber\\
\frac{\partial}{\partial x} f(h(x)) = \sum_{a=1}^D \sum_{b=1}^D \left[ \frac{\partial}{\partial h(x)} f(h(x)) \right]_{a,b} \left[ \frac{\partial}{\partial x} h(x) \right]_{a,b} \label{eq:diffrule3}
\end{eqnarray}
%
However, given that the prior and thus the posterior over $g$ does not have relevant gradient information outside of the unit interval, it is a better idea to use a combined sampler that proposes from Hamiltonian dynamics only for the dimensions of $v$.

\subsection{Gibbs sampling}

A more efficient way to collect samples from the joint posterior over all hidden variables is to employ a Gibbs sampling scheme, where we sample from the conditional posteriors. The first is over $v$, and can be defined as follows

\begin{equation} \label{eq:condpost}
p(v \mid x,g) = \frac{p(x \mid v,g) p(v \mid g)}{p(x \mid g)} = \frac{\mathcal{N}(x;Av,\sigma_x I) \mathcal{N}(v;0,C_v)}{\int_{-\infty}^{\infty} \mathcal{N}(x;Av,\sigma_x I) \mathcal{N}(v;0,C_v) \mathrm{d}v}
\end{equation}
%
The Gaussian over $x$ can be rewritten to a Gaussian over $v$ times a constant $c_1$ in the following way

\begin{equation}
\mathcal{N}(x;Av,\sigma_x I) = c_1 \mathcal{N}(v; -2 (A^T A)^{-1} A^T x, \sigma_x (A^T A)^{-1})
\end{equation}
%
Consequently, the product of two Gaussians in the numerator of Eq. \ref{eq:condpost} can also be written as a Gaussian over $v$ introducing a new constant

\begin{equation}
\mathcal{N}(x;Av,\sigma_x I) \mathcal{N}(v;0,C_v) = c_1 c_2 \mathcal{N}(v; \mu_{post},C_{post})
\end{equation}
%
The denominator of Eq. \ref{eq:condpost} is the integral of this formula, which evaluates to $c_1c_2$, as the Gaussian integrates to one. This cancels the constants in the numerator, making the conditional posterior equal to the combined Gaussian over $v$, which, after expanding $\mu_{post}$ and $C_{post}$, is

\begin{equation}
p(v \mid x,g) = \mathcal{N}\left(v; -\frac{2}{\sigma_x} \left(\frac{1}{\sigma_x} A^T A + C_v^{-1}\right)^{-1} A^T x, \left(\frac{1}{\sigma_x} A^T A + C_v^{-1}\right)^{-1}\right)
\end{equation}
%
and for a batch of size $B$

\begin{equation}
p(V \mid X,g) = \prod_{b=1}^B \mathcal{N} \left(v_b; \mu_{post}(x_b),C_{post} \right)
\end{equation}
%
which can be sampled directly from a Gaussian of dimension $Dv \times B$. The conditional posterior over $g$ is defined as follows

\begin{equation} 
p(g \mid X,V) = \frac{p(X \mid g,V) p(g \mid V)}{p(X \mid V)} = \frac{p(V \mid g) p(g)}{p(V)}
\end{equation}
%
which can be sampled by a Metropolis-Hastings or slice sampling scheme with the following target

\begin{equation} 
\log p(g \mid X,V) \sim -\frac{1}{2} \left[B\log(\det(C_v)) + \sum_{b=1}^B v_b^T C_v^{-1} v_b\right] + (\alpha-1) \sum_{k=1}^K \log(g_k)
\end{equation}

\section{M-step}

The complete-data likelihood with respect to a set of batch observations of size $B$, $\mathbf{X} = \lbrace X_1 \dots X_N \rbrace$ is the following

\begin{equation}
\begin{split}
&p(\mathbf{V},G,\mathbf{X} \mid C_{1..K}) = \prod_{n=1}^N p(X_n \mid V_n) p(V_n \mid g_n) p(g_n) = \\
&\prod_{n=1}^N p(g_n) \prod_{b=1}^B p(x_{nb} \mid v_{nb}) p(v_{nb} \mid g_n) 
\end{split}
\end{equation}
%
Its logarithm ($\mathcal{L}=\log p(\mathbf{V},G,\mathbf{X} \mid C_{1..K})$) is similar to \ref{eq:log_post}. We can approximate the integral of this logarithm over the joint posterior by averaging over $L$ samples from it, separately for each observation $x_n$. As we will seek the values of the covariance components $C_{1 \dots K}$ that maximise this integral, we can discard each term not depending on these parameters. This way we arrive to the following expression 

\begin{equation}
\mathcal{L} \sim \sum_{n=1}^N \frac{1}{L} \sum_{l=1}^L -\frac{1}{2} \left[B \log \left( \det \left( C_v^{l,n} \right) \right) + \sum_{b=1}^B v^{(l,n,b)T}  \left( C_v^{l,n} \right)^{-1} v^{l,n,b}\right]
\end{equation}
%
The double summation over $L$ samples over all $N$ observations always happens on the same terms, so we can substitute it with a single sum that iterates over the full sample set. So taking the derivative with respect to one of the covariance components $j \in \left[ 1,k \right]$, we get 

\begin{equation}\label{eq:deriv}
\frac{\partial \mathcal{L}}{\partial C_j} = -\frac{1}{2L} \sum_{m=1}^{LN} g_j^m \left[ B \left( C_v^m \right)^{-1} - \sum_{b=1}^B \left( C_v^m \right)^{-1} v^{m,b} v^{(m,b)T} \left( C_v^m \right)^{-1} \right]
\end{equation}
%
using $\frac{\partial}{\partial C_j} (C_v^m)^{-1} = \frac{\partial}{\partial C_j} \sum_{k=1}^K g_k^m C_k = g_j^m$ and the differentiating rules \ref{eq:diffrule1} and \ref{eq:diffrule2} (all $C_j$ matrices are symmetric and regular, so transposes are identical and determinants are nonzero).

Setting \ref{eq:deriv} to zero, substituting for $C_v$ and multiplying it from the left by $\sum_{k=1}^K g_k^m C_k$ gives

\begin{equation}\label{eq:zero}
0 =  -\frac{1}{2L} \sum_{m=1}^{LN}  g_j^m \left[ BI - \sum_{b=1}^B v^{m,b} v^{(m,b)T} \left( \sum_{k=1}^K g_k^m C_k \right)^{-1} \right]
\end{equation}
%
multiplying by $-2L$ and by $\sum_{k=1}^K g_k^m C_k$, but this time from the right, gives

\begin{equation}
0 = \sum_{m=1}^{LN} g_j^m \left[ B \left( \sum_{k=1}^K g_k^m C_k \right) - \sum_{b=1}^B v^{m,b} v^{(m,b)T} \right]
\end{equation}
%
rearranging the sums yields

\begin{equation}
0 =  \sum_{k=1}^K C_k B \left( \sum_{m=1}^{LN} g_j^m g_k^m \right)  - \sum_{m=1}^{LN} g_j^m \sum_{b=1}^B v^{m,b} v^{(m,b)T}
\end{equation}
%
From this we can express $C_j$ to get

\begin{equation}
C_j^{new} = \frac{1}{B \sum_{m=1}^{LN} g_j^{m2}} \left[ \sum_{m=1}^{LN} g_j^m \sum_{b=1}^B v^{m,b} v^{(m,b)T} - B \sum_{k \neq j}^K C_k \sum_{m=1}^{LN} g_j^m g_k^m \right]
\end{equation}
%
that may be rearranged to

\begin{equation}
C_j^{new} = \frac{1}{B \sum_{m=1}^{LN} g_j^{m2}} \sum_{m=1}^{LN} g_j^m \left[ \sum_{b=1}^B v^{m,b} v^{(m,b)T} - B \sum_{k \neq j}^K g_k^m C_k \right]
\end{equation}

\end{document}
