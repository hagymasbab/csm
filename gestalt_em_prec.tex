\documentclass{paper}
\usepackage{amsmath}
\usepackage{amssymb}
\DeclareMathOperator*{\argmin}{arg\,min}

\begin{document}

\title{EM for the gestalt model}
\maketitle

\section{Generative model}

A gestalt, a perceptual object, is characterised by a precision component for the joint distribution of visual neural activity. 

\begin{eqnarray}
p(v \mid g) = \mathcal{N}(v; 0,\Lambda_v^{-1}) \\
\Lambda_v = \sum_{k=1}^K g_k \Lambda_k \label{eq:cv}
\end{eqnarray}
%
where K is the fixed number of possible gestalts in the visual scene and $g_k$ is the strength of the gestalt number $k$, coming from a $K$-dimensional symmetric Dirichlet prior distribution with concentration parameter $\alpha$ controlling the sparsity of the prior.

\begin{equation}
p(g) = \textrm{Dir}(g; \alpha)
\end{equation}
%
The pixel intensities are generated from the neural activity through a set of linear projective field models, possibly Gabor filters, $A$, adding some independent observational noise.

\begin{eqnarray}
p(x \mid v) = \mathcal{N}(x; Av,C_x) \\
C_x = \sigma_x I;
\end{eqnarray}

We might assume that a single composition of gestalts, characterised by the $g$ vector, generates a batch of $B$ independent images, described by cellular activities $V = \lbrace v_1 \dots v_B \rbrace$ and observations $X = \lbrace x_1 \dots x_B \rbrace$. Then the following likelihood distributions hold

\begin{eqnarray}
p(V \mid g) = \prod_{b=1}^B \mathcal{N}(v_b; 0,\Lambda_v^{-1}) \\
p(X \mid V) = \prod_{b=1}^B \mathcal{N}(x_b; Av_b,C_x)
\end{eqnarray}

\section{Gibbs sampling as the E-step}

An efficient way to collect samples from the joint posterior over all hidden variables is to employ a Gibbs sampling scheme, where we sample from the conditional posteriors. The first is over $v$, and can be defined as follows

\begin{equation} \label{eq:condpost}
p(v \mid x,g) = \frac{p(x \mid v,g) p(v \mid g)}{p(x \mid g)} = \frac{\mathcal{N}(x;Av,\sigma_x I) \mathcal{N}(v;0,\Lambda_v^{-1})}{\int_{-\infty}^{\infty} \mathcal{N}(x;Av,\sigma_x I) \mathcal{N}(v;0,\Lambda_v^{-1}) \mathrm{d}v}
\end{equation}
%
The Gaussian over $x$ can be rewritten to a Gaussian over $v$ times a constant $c_1$ in the following way

\begin{equation}
\mathcal{N}(x;Av,\sigma_x I) = c_1 \mathcal{N}(v; -2 (A^T A)^{-1} A^T x, \sigma_x (A^T A)^{-1})
\end{equation}
%
Consequently, the product of two Gaussians in the numerator of Eq. \ref{eq:condpost} can also be written as a Gaussian over $v$ introducing a new constant

\begin{equation}
\mathcal{N}(x;Av,\sigma_x I) \mathcal{N}(v;0,\Lambda_v^{-1}) = c_1 c_2 \mathcal{N}(v; \mu_{post},C_{post})
\end{equation}
%
The denominator of Eq. \ref{eq:condpost} is the integral of this formula, which evaluates to $c_1c_2$, as the Gaussian integrates to one. This cancels the constants in the numerator, making the conditional posterior equal to the combined Gaussian over $v$, which, after expanding $\mu_{post}$ and $C_{post}$, is

\begin{equation}
p(v \mid x,g) = \mathcal{N}\left(v; -\frac{2}{\sigma_x} \left(\frac{1}{\sigma_x} A^T A + \Lambda_v \right)^{-1} A^T x, \left(\frac{1}{\sigma_x} A^T A + \Lambda_v\right)^{-1}\right)
\end{equation}
%
and for a batch of size $B$

\begin{equation}
p(V \mid X,g) = \prod_{b=1}^B \mathcal{N} \left(v_b; \mu_{post}(x_b),C_{post} \right)
\end{equation}
%
which can be sampled directly from a Gaussian of dimension $D_v \times B$. The conditional posterior over $g$ is defined as follows

\begin{equation} 
p(g \mid X,V) = \frac{p(X \mid g,V) p(g \mid V)}{p(X \mid V)} = \frac{p(V \mid g) p(g)}{p(V)}
\end{equation}
%
which can be sampled by a slice sampling scheme with the following target

\begin{equation} 
\log p(g \mid X,V) \sim -\frac{1}{2} \left[B\log(\det(\Lambda_v^{-1})) + \sum_{b=1}^B v_b^T \Lambda_v v_b\right] + (\alpha-1) \sum_{k=1}^K \log(g_k)
\end{equation}

\section{M-step}

The complete-data likelihood with respect to a set of batch observations of size $B$, $\mathbf{X} = \lbrace X_1 \dots X_N \rbrace$ is the following

\begin{equation}
\begin{split}
&p(\mathbf{V},G,\mathbf{X} \mid \Lambda_{1..K}) = \prod_{n=1}^N p(X_n \mid V_n) p(V_n \mid g_n) p(g_n) = \\
&\prod_{n=1}^N p(g_n) \prod_{b=1}^B p(x_{nb} \mid v_{nb}) p(v_{nb} \mid g_n) 
\end{split}
\end{equation}
%
Let's denote the logarithm of this by $\mathcal{L}=\log p(\mathbf{V},G,\mathbf{X} \mid \Lambda_{1..K})$. We can approximate the integral of this logarithm over the joint posterior by averaging over $L$ samples from it, separately for each observation $x_n$. As we will seek the values of the precision components $\Lambda_{1 \dots K}$ that maximise this integral, we can discard each term not depending on these parameters. This way we arrive to the following expression 

\begin{equation}
\begin{split}
\mathcal{L} \sim \sum_{n=1}^N \frac{1}{L} \sum_{l=1}^L -\frac{1}{2} \left[B \log \left( \det \left( \Lambda_v^{-(l,n)} \right) \right) + \sum_{b=1}^B v^{(l,n,b)T}  \Lambda_v^{l,n} v^{l,n,b}\right]& = \\
= -\frac{1}{2L} \sum_{m=1}^{NL} \left[B \log \left( \det \left( \Lambda_v^{-m} \right) \right) + \sum_{b=1}^B v^{(m,b)T}  \Lambda_v^{m} v^{m,b}\right]&
\end{split}
\end{equation}
%
noting that the double summation over $L$ samples over all $N$ observations always happens on the same terms, so we can substitute it with a single sum that iterates over the full sample set.

To ensure that the optimisation procedure does not produce precision matrices that are not positive definite, we can optimise for the Cholesky upper triangle matrix istead of the precision matrix, as this also specifies the Gaussian completely.

\begin{eqnarray}
\Lambda_j = U_j^T U_j \\
\Lambda_v = \sum_{k=1}^K g_k U_k^T U_k \\
\frac{\partial \Lambda_v}{\partial U_j} = 2 g_j U_j
\end{eqnarray}
%
so by the chain rule, the derivative of $\mathcal{L}$ according to $U_j$ looks like this

\begin{equation}
\begin{split}
&\frac{\partial \mathcal{L}}{\partial U_j} = -\frac{1}{L} \sum_{m=1}^{LN} \frac{\partial \mathcal{L}^m}{\partial \Lambda_v^m} \frac{\partial \Lambda_v^m}{\partial U_j} = \\
& -\frac{1}{L} \sum_{m=1}^{LN} g_j^{m2} \left[ - B \Lambda_v^{-m} + \sum_{b=1}^B v^{m,b} v^{(m,b)T} \right] U_j
\end{split}
\end{equation}

\end{document}
