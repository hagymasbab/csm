\documentclass{paper}
\usepackage{amsmath}
\usepackage{amssymb}
\DeclareMathOperator*{\argmin}{arg\,min}
\DeclareMathOperator*{\argmax}{arg\,max}

\begin{document}

\title{Gradient ascent for maximum likelihood estimation in the CSM model}
\maketitle

\begin{equation}
p(X \mid C_{1..K}) = \prod_{n=1}^N p(x_n \mid C_{1..K}) 
\end{equation}

\begin{eqnarray}
p(X \mid C_{1..K}) = \prod_{n=1}^N \iint_{-\infty}^{\infty} p(x_n \mid z,g) p(g) p(z) \mathrm{d}g\mathrm{d}z \\
%p(X \mid C_{1..K}) = \prod_{n=1}^N \iiint_{-\infty}^{\infty} p(x_n,v_n \mid z_n,g_n) p(g_n) p(z_n) \mathrm{d}v_n\mathrm{d}g_n\mathrm{d}z_n
\end{eqnarray}
%
approximation of some of the integrals by samples from the priors $p(g_n)$ and $p(z_n)$

\begin{equation}
p(X \mid C_{1..K}) \approx \prod_{n=1}^N \sum_{l=1}^{L}p(x_n \mid z^l,g^l)
\end{equation}
%
the integral evaluates as follows, according to M\'at\'e Lengyel's intuition, and by the lengthy algebraic manipulations we arrive to a form in which the dependence of covariance matrix on components is simpler, leading to a simpler derivative

\begin{eqnarray}
\begin{split}
p(x \mid z,g) = \int_{-\infty}^{\infty} p(x,v \mid z,g)  \mathrm{d}v = \mathcal{N}(x;0,\sigma_x I + z^2 A \left( \sum_{k=1}^K g_k U_k^T U_k \right)A^T) =\\
= \frac{1}{z^{D_v} \sqrt{\det(A^TA)}} \mathcal{N}(\frac{1}{z}A^{+}x;0,\frac{\sigma_x}{z^2} (A^TA)^{-1} + \sum_{k=1}^K g_k U_k^T U_k) 
%= \mathcal{N}(x;0,\sigma_x I + z^2 A \left( \sum_{k=1}^K g_k U_k^T U_k \right)A^T) 
\end{split} \\
f(x,z) \equiv \frac{1}{z}A^{+}x, ~ h(z) \equiv  \frac{1}{z^{D_v} } \\
C(z,g) \equiv \frac{\sigma_x}{z^2} (A^TA)^{-1} + \sum_{k=1}^K g_k U_k^T U_k \\
%C(z,g) \equiv \sigma_x I + z^2 A \left( \sum_{k=1}^K g_k U_k^T U_k \right)A^T \\
\end{eqnarray}
%
thus, the likelihood can be expressed as

\begin{eqnarray}
p(X \mid C_{1..K}) \approx \det(A^TA)^{-\frac{N}{2}} \prod_{n=1}^N \sum_{l=1}^{L} h(z^l) \mathcal{N}(f(x_n,z^l);0,C(z^l,g^l)) \\
\log p(X \mid C_{1..K}) \approx -\frac{N}{2} \log(\det(A^TA)) + \sum_{n=1}^N \log \left[ \sum_{l=1}^{L} h(z^l) \mathcal{N}(f(x_n,z^l);0,C(z^l,g^l)) \right] \\
h^l \equiv h(z^l), ~ f_n^l \equiv f(x_n,z^l), ~ C^l \equiv C(z^l,g^l) \\
\mathcal{L}_n^l \equiv h^l \mathcal{N}(f_n^l;0,C^l), ~ \mathcal{L}_n \equiv  \sum_{l=1}^{L} \mathcal{L}_n^l \\
\log p(X \mid C_{1..K}) \approx \sum_{n=1}^N \log \mathcal{L}_n
\end{eqnarray}
%
the derivative of the likelihood with respect to a single element of the Cholesky decomposition of one of the covariance components can be decomposed this way

\begin{equation}
\begin{split}
\frac{\partial \log p(X \mid C_{1..K})}{\partial \left[ U_k \right]_{i,j}} \approx \sum_{n=1}^N \frac{\partial \log \mathcal{L}_n}{\partial \left[ U_k \right]_{i,j}} = \sum_{n=1}^N \frac{1}{\mathcal{L}_n} \frac{\partial \mathcal{L}_n}{\partial \left[ U_k \right]_{i,j}} = \sum_{n=1}^N \frac{1}{\mathcal{L}_n}  \sum_{l=1}^{L} \frac{\partial \mathcal{L}_n^l}{\partial \left[ U_k \right]_{i,j}} = \\
=  \sum_{n=1}^N \frac{1}{\mathcal{L}_n}  \sum_{l=1}^{L} \textrm{Tr} \left[ \frac{\partial \mathcal{L}_n^l}{\partial C^l} \frac{\partial C^l}{\partial \left[ U_k \right]_{i,j}} \right]
\end{split}
\end{equation}
% 
the derivatives in this formula are the following

\begin{eqnarray}
\begin{split}
\frac{\partial \mathcal{L}_n^l}{\partial C^l} = h^l \frac{\partial}{\partial C^l} \mathcal{N}(f_n^l;0,C^l) = h^l \mathcal{N}(f_n^l;0,C^l) \frac{\partial}{\partial C^l} \log \mathcal{N}(f_n^l;0,C^l) = \\
= -\frac{h^l}{2} \mathcal{N}(f_n^l;0,C^l) \left[ (C^l)^{-1} - (C^l)^{-1} f_n^l (f_n^l)^T (C^l)^{-1} \right]
\end{split} \\
\frac{\partial C(z,g)}{\partial \left[ U_k \right]_{i,j}} = g_k \frac{\partial \left( U_k^T U_k \right)}{\partial \left[ U_k \right]_{i,j}} = g_k \left( U_k^T J^{ij} + J^{ji} U_k \right) \equiv g_k \hat U_k^{ij}
\end{eqnarray}
%
substituting back to the derivative

\begin{equation}
\begin{split}
\frac{\partial \log p(X \mid C_{1..K})}{\partial \left[ U_k \right]_{i,j}} \approx \\
\approx  -\frac{1}{2} \sum_{n=1}^N \frac{1}{\mathcal{L}_n}  \sum_{l=1}^{L} h^l g^l_k \mathcal{N}(f_n^l;0,C^l)  \textrm{Tr} \left[ \left[ (C^l)^{-1} - (C^l)^{-1} f_n^l (f_n^l)^T (C^l)^{-1} \right] \hat U_k^{ij} \right] = \\
= -\frac{1}{2} \textrm{Tr} \left[ \sum_{n=1}^N \left(\frac{1}{\mathcal{L}_n}  \sum_{l=1}^{L} h^l g^l_k \mathcal{N}(f_n^l;0,C^l)   \left[ (C^l)^{-1} - (C^l)^{-1} f_n^l (f_n^l)^T (C^l)^{-1} \right] \right) \hat U_k^{ij} \right]
\end{split}
\end{equation}
%
The regularities of the $\hat U_k$ matrices allow us to replace the trace with a much more efficient computation:

\begin{eqnarray}
M_k = -\frac{1}{2} \sum_{n=1}^N \left(\frac{1}{\mathcal{L}_n}  \sum_{l=1}^{L} h^l g^l_k \mathcal{N}(f_n^l;0,C^l)   \left[ (C^l)^{-1} - (C^l)^{-1} f_n^l (f_n^l)^T (C^l)^{-1} \right] \right) \\
\frac{\partial \log p(X \mid C_{1..K})}{\partial \left[ U_k \right]_{i,j}} \approx \textrm{Tr} \left[M_k \hat U_k^{ij} \right] = \sum_{a=1}^{Dv} \left[ M_k \right]_{j,a} \left[ U_k \right]_{i,a} + \left[ M_k \right]_{a,j} \left[ U_k \right]_{i,a}
\end{eqnarray}
%
we can move the parameters in the direction of the gradient scaled by a learning rate

\begin{equation}
\left[ U_k \right]_{i,j} \leftarrow \left[ U_k \right]_{i,j} + \epsilon \frac{\partial \log p(X \mid C_{1..K})}{\partial \left[ U_k \right]_{i,j}}
\end{equation}

\section{MAP estimate of $g$}

\begin{eqnarray}
g_{MAP} = \argmax_g p(g \mid x) = \argmax_g \frac{p(x \mid g)p(g)}{p(x)} = \argmax_g p(x \mid g)p(g)\\
p(x \mid g) = \int_{-\infty}^{\infty} p(x \mid z,g)p(z)\mathrm{d}z \approx \frac{1}{L} \sum_{l=1}^{L} p(x \mid g,z^l)
\end{eqnarray}

\end{document}
